{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_simple_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoheiFukuhara/bert_nlp/blob/main/section_2/03_simple_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv454lBK1YCc"
      },
      "source": [
        "# シンプルなBERTの実装\n",
        "訓練済みのモデルを使用し、文章の一部の予測、及び2つの文章が連続しているかどうかの判定を行います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_Ozfz3NhltP"
      },
      "source": [
        "## ライブラリのインストール\n",
        "PyTorch-Transformers、および必要なライブラリのインストールを行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_mDYVlb-sqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f4ace31-d138-4969-8537-5591163e9072"
      },
      "source": [
        "!pip install folium==0.2.1\n",
        "!pip install urllib3==1.25.11\n",
        "!pip install pytorch-transformers==1.2.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 20 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 40 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 69 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=1a441676029a6a933d07d4c151c387e8f2195a9edd205a6e1d40ceea508d7fa0\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n",
            "Collecting urllib3==1.25.11\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed urllib3-1.25.11\n",
            "Collecting pytorch-transformers==1.2.0\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (1.21.5)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.21.3-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 24.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (2019.12.20)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (1.10.0+cu111)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 37.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (3.10.0.2)\n",
            "Collecting botocore<1.25.0,>=1.24.3\n",
            "  Downloading botocore-1.24.3-py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 40.2 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.1-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.3->boto3->pytorch-transformers==1.2.0) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.3->boto3->pytorch-transformers==1.2.0) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.3->boto3->pytorch-transformers==1.2.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (1.1.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, pytorch-transformers\n",
            "Successfully installed boto3-1.21.3 botocore-1.24.3 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.5.1 sacremoses-0.0.47 sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb5gVyYF2vjL"
      },
      "source": [
        "## 文章の一部の予測\n",
        "文章における一部の単語をMASKし、それをBERTのモデルを使って予測します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6oFZnS21Mqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3044786c-9358-475c-df93-682d66b8045e"
      },
      "source": [
        "import torch\n",
        "from pytorch_transformers import BertForMaskedLM\n",
        "from pytorch_transformers import BertTokenizer\n",
        "\n",
        "\n",
        "text = \"[CLS] I played baseball with my friends at school yesterday [SEP]\"\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "words = tokenizer.tokenize(text)\n",
        "print(words)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 3212876.89B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'i', 'played', 'baseball', 'with', 'my', 'friends', 'at', 'school', 'yesterday', '[SEP]']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Y32Tl55dSl"
      },
      "source": [
        "文章の一部をMASKします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_H50V7b5RM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63adcdd3-3453-4419-9da1-8d2a74145469"
      },
      "source": [
        "msk_idx = 3\n",
        "words[msk_idx] = \"[MASK]\"  # 単語を[MASK]に置き換える\n",
        "print(words)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'i', 'played', '[MASK]', 'with', 'my', 'friends', 'at', 'school', 'yesterday', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBXgAn9s528_"
      },
      "source": [
        "単語を対応するインデックスに変換します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm4qbMPW56-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028b3edd-d764-40b3-8702-e0572b945e35"
      },
      "source": [
        "word_ids = tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
        "print(word_ids)\n",
        "word_tensor = torch.tensor([word_ids])  # テンソルに変換\n",
        "print(word_tensor)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 1045, 2209, 103, 2007, 2026, 2814, 2012, 2082, 7483, 102]\n",
            "tensor([[ 101, 1045, 2209,  103, 2007, 2026, 2814, 2012, 2082, 7483,  102]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y76O88877cB_"
      },
      "source": [
        "BERTのモデルを使って予測を行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2NWREc77gQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e3d1cb-2f35-426f-e391-c471442c46d7"
      },
      "source": [
        "%%time\n",
        "msk_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "msk_model.cuda()  # GPU対応\n",
        "msk_model.eval()  # 学習しないので評価モデルに設定\n",
        "\n",
        "x = word_tensor.cuda()  # GPU対応\n",
        "y = msk_model(x)  # 予測\n",
        "print(type(y))\n",
        "result = y[0] # タプル形式の0を取得\n",
        "print(result.size())  # 結果の形状\n",
        "\n",
        "_, max_ids = torch.topk(result[0][msk_idx], k=5)  # 最も大きい(確率が高い)5つの値\n",
        "result_words = tokenizer.convert_ids_to_tokens(max_ids.tolist())  # インデックスを単語に変換\n",
        "print(result_words)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tuple'>\n",
            "torch.Size([1, 11, 30522])\n",
            "['basketball', 'football', 'soccer', 'baseball', 'tennis']\n",
            "CPU times: user 3.82 s, sys: 229 ms, total: 4.05 s\n",
            "Wall time: 4.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Su6QTCAAgFS"
      },
      "source": [
        "## 文章が連続しているかどうかの判定\n",
        "BERTのモデルを使って、2つの文章が連続しているかどうかの判定を行います。  \n",
        "以下の関数`show_continuity`では、2つの文章の連続性を判定し、表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC0nihWMAtgG"
      },
      "source": [
        "from pytorch_transformers import BertForNextSentencePrediction\n",
        "\n",
        "def show_continuity(text, seg_ids):\n",
        "    words = tokenizer.tokenize(text)\n",
        "    word_ids = tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
        "    word_tensor = torch.tensor([word_ids])  # テンソルに変換\n",
        "\n",
        "    seg_tensor = torch.tensor([seg_ids])\n",
        "\n",
        "    nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "    nsp_model.cuda()  # GPU対応\n",
        "    nsp_model.eval()\n",
        "\n",
        "    x = word_tensor.cuda()  # GPU対応\n",
        "    s = seg_tensor.cuda()  # GPU対応\n",
        "\n",
        "    y = nsp_model(x, s)  # 予測\n",
        "    result = torch.softmax(y[0], dim=1)\n",
        "    print(result)  # Softmaxで確率に\n",
        "    print(str(result[0][0].item()*100) + \"%の確率で連続しています。\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWogb8nFIQMg"
      },
      "source": [
        "`show_continuity`関数に、自然につながる2つの文章を与えます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaUmof1yF_rD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d710da-071d-44b0-e24f-9259f88cf5d8"
      },
      "source": [
        "text = \"[CLS] What is baseball ? [SEP] It is a game of hitting the ball with the bat [SEP]\"\n",
        "seg_ids = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1, 1]  # 0:前の文章の単語、1:後の文章の単語\n",
        "show_continuity(text, seg_ids)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 4.5869e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "99.9995470046997%の確率で連続しています。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKjotaOCIeeK"
      },
      "source": [
        "`show_continuity`関数に、自然につながらない2つの文章を与えます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4qAKBlcGRYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63af2484-e24f-4063-9b34-8372b6c4d001"
      },
      "source": [
        "text = \"[CLS] What is baseball ? [SEP] This food is made with flour and milk [SEP]\"\n",
        "seg_ids = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 0:前の文章の単語、1:後の文章の単語\n",
        "show_continuity(text, seg_ids)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[9.5296e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "0.000952963819145225%の確率で連続しています。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ProaZBR89f_J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}